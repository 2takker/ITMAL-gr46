{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ITMAL Exercise\n",
    "\n",
    "REVISIONS|\n",
    "---------|------------------------------------------------\n",
    "2018-1219| CEF, initial.                  \n",
    "2018-0206| CEF, updated and spell checked. \n",
    "2018-0208| CEF, minor text updata. \n",
    "\n",
    "## Implementing a dummy classifier with fit-predict interface\n",
    "\n",
    "We begin with the MNIST data-set and will reuse the data loader you just created. Next we create a dummy classifier, and compare the results of the SGD and dummy classifiers using the MNIST data...\n",
    "\n",
    "#### Qa  Add a Stochastic Gradient Decent [SGD] Classifier\n",
    "\n",
    "Create a train-test data-set for MNIST and then add the `SGDClassifier` as done in [HOLM], p82.\n",
    "\n",
    "Split your data and run the fit-predict for the classifier using the MNIST data.\n",
    "\n",
    "Notice that you have to reshape the MNIST X-data to be able to use the classifier. It may be a 3D array, consisting of 70000 (28 x 28) images, or just a 2D array consisting of 70000 elements of size 784.\n",
    "\n",
    "A simple `reshape()` could fix this on-the-fly:\n",
    "```python\n",
    "X, y = MNIST_GetDataSet()\n",
    "\n",
    "print(\"X.shape=\",X.shape) # print X.shape= (70000, 28, 28)\n",
    "if X.ndim==3:\n",
    "    print(\"reshaping X..\")\n",
    "    assert y.ndim==1\n",
    "    X = X.reshape((X.shape[0],X.shape[1]*X.shape[2]))\n",
    "assert X.ndim==2\n",
    "print(\"X.shape=\",X.shape) # X.shape= (70000, 784)\n",
    "```\n",
    "\n",
    "Remember to use the category-5 y inputs\n",
    "\n",
    "```python\n",
    "y_train_5 = (y_train == 5)    \n",
    "y_test_5  = (y_test == 5)\n",
    "```\n",
    "instead of the `y`'s you are getting out of the dataloader...\n",
    "\n",
    "Test your model on using the test data, and try to plot numbers that have been categorized correctly. Then also find and plots some misclassified numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape= (70000, 28, 28)\n",
      "reshaping X..\n",
      "X.shape= (70000, 784)\n",
      "[[15592   364]\n",
      " [  255  1289]]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Qa...\n",
    "\n",
    "#not best practise...\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "import sys, os\n",
    "import numpy as np\n",
    "sys.path.append(os.path.join(os.path.curdir, os.path.pardir))\n",
    "from libitmal.dataloaders import MNIST_GetDataSet\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "X, y = MNIST_GetDataSet()\n",
    "\n",
    "\n",
    "print(\"X.shape=\",X.shape) # print X.shape= (70000, 28, 28)\n",
    "if X.ndim==3:\n",
    "    print(\"reshaping X..\")\n",
    "    assert y.ndim==1\n",
    "    X = X.reshape((X.shape[0],X.shape[1]*X.shape[2]))\n",
    "assert X.ndim==2\n",
    "print(\"X.shape=\",X.shape) # X.shape= (70000, 784)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "\n",
    "y_train_5 = (y_train == 5)    \n",
    "y_test_5  = (y_test == 5)\n",
    "\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "sgd_clf.fit(X_train,y_train_5)\n",
    "\n",
    "y_test_predict = sgd_clf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cf = confusion_matrix(y_test_5,y_test_predict)\n",
    "print(cf)\n",
    "\n",
    "from libitmal.dataloaders import MNIST_PlotDigit\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "\n",
    "#i = 0\n",
    "#while i < 5:\n",
    "   # print(\"ok\")\n",
    "    #if y_test_5[i] == y_test_predict[i]:\n",
    "        #MNIST_PlotDigit(X_test[i])\n",
    "        #i+=1\n",
    "#plt.show()\n",
    "#print(\"ok\")\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qb Implement a dummy binary classifier\n",
    "\n",
    "Follow the code found in [HOML], p84, but name you estimator `DummyClassifier` instead of `Never5Classifyer`.\n",
    "\n",
    "Here our Python class knowledge comes into play. The estimator class hierarchy looks like\n",
    "\n",
    "<img src=\"Figs/class_base_estimator.png\" style=\"width:500px\">\n",
    "\n",
    "All Scikit-learn classifiers inherit form `BaseEstimator` (and possible also `ClassifierMixin`), and they must have a `fit-predict` function pair (strangely not in the base class!) and you can actually find the `sklearn.base.BaseEstimator` and `sklearn.base.ClassifierMixin` python source code somewhere in you anaconda install dir, if you should have the nerves to go to such interesting details.\n",
    "\n",
    "But surprisingly you may just want to implement a class that contains the `fit-predict` functions, ___without inheriting___ from the `BaseEstimator`, things still work due to the pythonic 'duck-typing': you just need to have the class implement the needed interfaces, obviously `fit()` and `predict()` but also the more obscure `get_params()` etc....then the class 'looks like' a `BaseEstimator`...and if it looks like an estimator, it _is_ an estimator (aka. duct typing).\n",
    "\n",
    "Templates in C++ also allow the language to use compile-time duck typing!\n",
    "\n",
    "> https://en.wikipedia.org/wiki/Duck_typing\n",
    "\n",
    "Call the fit-predict on a newly instantiated `DummyClassifier` object, and try to compare the confusion matrix for both the dummy and SDG classifier.\n",
    "\n",
    "We will be discussing the confusion matrix next, but first, print the `y_test_5.shape` and count the numbers of `y_test_5==True` and `y_test_5==False` and see if you can find these numbers in the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DummyClassifier\n",
      "c= [0.91566678 0.90896623 0.91068061]\n",
      "M=[[15956     0]\n",
      "   [ 1544     0]]\n",
      "SGD classifier\n",
      "c= [0.96606102 0.9026397  0.96141975]\n",
      "M=[[15592   364]\n",
      "   [  255  1289]]\n",
      "y_test_5.shape = 17500\n",
      "Number of y_test_5==True:1544\n",
      "Number of y_test_5==False:15956\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from libitmal import utils\n",
    "\n",
    "# TODO: Qb\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class DummyClassifier(BaseEstimator):\n",
    "    def fit(self,X,y=None):\n",
    "        pass\n",
    "    def predict(self,X):\n",
    "        return np.zeros((len(X),1),dtype=bool)\n",
    "\n",
    "dummy_classifier = DummyClassifier()\n",
    "dummy_classifier.fit(X_train)\n",
    "\n",
    "y_dummy_predicted = dummy_classifier.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Inspiration for running cross evaluation on a model, and printing the confusion matrix:\n",
    "# c=cross_val_score(model, X_test, y_test_5, cv=3, scoring=\"accuracy\")\n",
    "# print(\"c=\",c)\n",
    "#M=confusion_matrix(y_test_true, y_test_pred)\n",
    "# itmalutils.PrintMatrix(M,\"M=\")\n",
    "\n",
    "print(\"DummyClassifier\")\n",
    "cd=cross_val_score(dummy_classifier, X_test, y_test_5, cv=3, scoring=\"accuracy\")\n",
    "print(\"c=\",cd)\n",
    "M=confusion_matrix(y_test_5, y_dummy_predicted)\n",
    "utils.PrintMatrix(M,\"M=\")\n",
    "\n",
    "print(\"SGD classifier\")\n",
    "csgd=cross_val_score(sgd_clf, X_test, y_test_5, cv=3, scoring=\"accuracy\")\n",
    "print(\"c=\",csgd)\n",
    "M=confusion_matrix(y_test_5, y_test_predict)\n",
    "utils.PrintMatrix(M,\"M=\")\n",
    "\n",
    "#This shows why accuracy wouldnt be the preffered performance measure for classifiers,\n",
    "#as the you both would be able to figure out from the dummy classifiers code, but also see from the confusion matricies,\n",
    "#which clearly show that there arent any true-positives in the dummy-classfier, among other relevant information.\n",
    "\n",
    "ytestshape = y_test_5.shape[0]\n",
    "print(f\"y_test_5.shape = {ytestshape}\")\n",
    "true = np.count_nonzero(y_test_5 == True)\n",
    "print(f\"Number of y_test_5==True:{true}\")\n",
    "false = ytestshape - true\n",
    "print(f\"Number of y_test_5==False:{false}\")\n",
    "#which obviously matches perfectly with the results of the dummyclassifier..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
